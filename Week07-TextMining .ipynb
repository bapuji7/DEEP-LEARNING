{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76987f70",
   "metadata": {},
   "source": [
    " Week07-TextMining "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44119987",
   "metadata": {},
   "source": [
    "#### Team members : \n",
    " Bapuji Satyala (U61600601)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c415a6f",
   "metadata": {},
   "source": [
    "In this Jupyter notebook, we have used the dataset from UCI repository \" User review setiment Analysis \" where our data consists of 3 files from 3 sources imdb,yelp and amazon. we have combined the 3 files into a single dataframe and performed text analysis on the same. Initially we haved applied text lemmetization to reduce the words to the base form, post that data is split to undergo TFID vectorization. TFIDF vectorization includes preprocessing of data,tokenisation and filtering stop words. the data undergoes Single value decompositon to lessen the chance of overfitting and speed up computation and fit into the respective model thus acheiving the performance metrics using Hyoerparameter tuning to acheive the best model analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3791c2de",
   "metadata": {},
   "source": [
    "#### Importing the dataset from reliable source like UCI Repository."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c09c5e",
   "metadata": {},
   "source": [
    "#### Importing all the necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cda0cb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk import pos_tag, word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn import preprocessing \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "random_seed = 42  # set random seed to ensure the results are repeatable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95947ff2",
   "metadata": {},
   "source": [
    " Our dataset contains user reviews from three different sources, lets combine them first to perform an analysis on the combined dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e99c8d19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[                                              sentence  label\n",
       " 0                             Wow... Loved this place.      1\n",
       " 1                                   Crust is not good.      0\n",
       " 2            Not tasty and the texture was just nasty.      0\n",
       " 3    Stopped by during the late May bank holiday of...      1\n",
       " 4    The selection on the menu was great and so wer...      1\n",
       " ..                                                 ...    ...\n",
       " 995  I think food should have flavor and texture an...      0\n",
       " 996                           Appetite instantly gone.      0\n",
       " 997  Overall I was not impressed and would not go b...      0\n",
       " 998  The whole experience was underwhelming, and I ...      0\n",
       " 999  Then, as if I hadn't wasted enough of my life ...      0\n",
       " \n",
       " [1000 rows x 2 columns],\n",
       "                                               sentence  label\n",
       " 0    A very, very, very slow-moving, aimless movie ...      0\n",
       " 1    Not sure who was more lost - the flat characte...      0\n",
       " 2    Attempting artiness with black & white and cle...      0\n",
       " 3         Very little music or anything to speak of.        0\n",
       " 4    The best scene in the movie was when Gerardo i...      1\n",
       " ..                                                 ...    ...\n",
       " 743  I just got bored watching Jessice Lange take h...      0\n",
       " 744  Unfortunately, any virtue in this film's produ...      0\n",
       " 745                   In a word, it is embarrassing.        0\n",
       " 746                               Exceptionally bad!        0\n",
       " 747  All in all its an insult to one's intelligence...      0\n",
       " \n",
       " [748 rows x 2 columns],\n",
       "                                               sentence  label\n",
       " 0    So there is no way for me to plug it in here i...      0\n",
       " 1                          Good case, Excellent value.      1\n",
       " 2                               Great for the jawbone.      1\n",
       " 3    Tied to charger for conversations lasting more...      0\n",
       " 4                                    The mic is great.      1\n",
       " ..                                                 ...    ...\n",
       " 995  The screen does get smudged easily because it ...      0\n",
       " 996  What a piece of junk.. I lose more calls on th...      0\n",
       " 997                       Item Does Not Match Picture.      0\n",
       " 998  The only thing that disappoint me is the infra...      0\n",
       " 999  You can not answer calls with the unit, never ...      0\n",
       " \n",
       " [1000 rows x 2 columns]]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "file_names = ['yelp_labelled.txt','imdb_labelled.txt','amazon_cells_labelled.txt']\n",
    "df_list=[]\n",
    "\n",
    "for file_name in file_names:\n",
    "    df_list.append(pd.read_csv(file_name, names=['sentence', 'label'], sep='\\t'))\n",
    "\n",
    "df_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee50178",
   "metadata": {},
   "source": [
    "In above code reads labeled phrases from three text files (yelp_labelled.txt, imdb_labelled.txt, and amazon_cells_labelled.txt) into pandas DataFrames. Sentence and Label are the two columns that make up each DataFrame. The content of the sentence is located in the'sentence' column, while the binary label designating whether the sentence is positive (1) or negative (0) is located in the 'label' column. The DataFrames are then kept by the code in a list named df_list, where each entry points to a different input file. This method works well for loading several datasets into memory for preprocessing or additional analysis. Putting data in a structured format, such as pandas DataFrames, makes it easier to manipulate and analyze; this is a typical technique in data science and machine learning operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "56bbc7a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wow... Loved this place.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Crust is not good.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Not tasty and the texture was just nasty.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stopped by during the late May bank holiday of...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The selection on the menu was great and so wer...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>The screen does get smudged easily because it ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>What a piece of junk.. I lose more calls on th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>Item Does Not Match Picture.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>The only thing that disappoint me is the infra...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>You can not answer calls with the unit, never ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2748 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              sentence  label\n",
       "0                             Wow... Loved this place.      1\n",
       "1                                   Crust is not good.      0\n",
       "2            Not tasty and the texture was just nasty.      0\n",
       "3    Stopped by during the late May bank holiday of...      1\n",
       "4    The selection on the menu was great and so wer...      1\n",
       "..                                                 ...    ...\n",
       "995  The screen does get smudged easily because it ...      0\n",
       "996  What a piece of junk.. I lose more calls on th...      0\n",
       "997                       Item Does Not Match Picture.      0\n",
       "998  The only thing that disappoint me is the infra...      0\n",
       "999  You can not answer calls with the unit, never ...      0\n",
       "\n",
       "[2748 rows x 2 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.concat(df_list)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6c214cf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentence    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['sentence']].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44725ec",
   "metadata": {},
   "source": [
    "Checking the null values in Sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257c040c",
   "metadata": {},
   "source": [
    "## Perform text lemmatization to reduce words to their base or root form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1ca2c577",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>transformed_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wow... Loved this place.</td>\n",
       "      <td>1</td>\n",
       "      <td>Wow ... Loved this place .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Crust is not good.</td>\n",
       "      <td>0</td>\n",
       "      <td>Crust be not good .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Not tasty and the texture was just nasty.</td>\n",
       "      <td>0</td>\n",
       "      <td>Not tasty and the texture be just nasty .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stopped by during the late May bank holiday of...</td>\n",
       "      <td>1</td>\n",
       "      <td>Stopped by during the late May bank holiday of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The selection on the menu was great and so wer...</td>\n",
       "      <td>1</td>\n",
       "      <td>The selection on the menu be great and so be t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>The screen does get smudged easily because it ...</td>\n",
       "      <td>0</td>\n",
       "      <td>The screen do get smudge easily because it tou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>What a piece of junk.. I lose more calls on th...</td>\n",
       "      <td>0</td>\n",
       "      <td>What a piece of junk .. I lose more call on th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>Item Does Not Match Picture.</td>\n",
       "      <td>0</td>\n",
       "      <td>Item Does Not Match Picture .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>The only thing that disappoint me is the infra...</td>\n",
       "      <td>0</td>\n",
       "      <td>The only thing that disappoint me be the infra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>You can not answer calls with the unit, never ...</td>\n",
       "      <td>0</td>\n",
       "      <td>You can not answer call with the unit , never ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2748 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              sentence  label  \\\n",
       "0                             Wow... Loved this place.      1   \n",
       "1                                   Crust is not good.      0   \n",
       "2            Not tasty and the texture was just nasty.      0   \n",
       "3    Stopped by during the late May bank holiday of...      1   \n",
       "4    The selection on the menu was great and so wer...      1   \n",
       "..                                                 ...    ...   \n",
       "995  The screen does get smudged easily because it ...      0   \n",
       "996  What a piece of junk.. I lose more calls on th...      0   \n",
       "997                       Item Does Not Match Picture.      0   \n",
       "998  The only thing that disappoint me is the infra...      0   \n",
       "999  You can not answer calls with the unit, never ...      0   \n",
       "\n",
       "                                  transformed_sentence  \n",
       "0                          Wow ... Loved this place .   \n",
       "1                                 Crust be not good .   \n",
       "2           Not tasty and the texture be just nasty .   \n",
       "3    Stopped by during the late May bank holiday of...  \n",
       "4    The selection on the menu be great and so be t...  \n",
       "..                                                 ...  \n",
       "995  The screen do get smudge easily because it tou...  \n",
       "996  What a piece of junk .. I lose more call on th...  \n",
       "997                     Item Does Not Match Picture .   \n",
       "998  The only thing that disappoint me be the infra...  \n",
       "999  You can not answer call with the unit , never ...  \n",
       "\n",
       "[2748 rows x 3 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed_corpus = []\n",
    "wnl = WordNetLemmatizer()\n",
    "for document in df['sentence']:\n",
    "    transformed_document = \"\"\n",
    "    for word, tag in pos_tag(word_tokenize(document)):\n",
    "        wntag = tag[0].lower()\n",
    "        wntag = wntag if wntag in ['a', 'r', 'n', 'v'] else None\n",
    "        if not wntag:\n",
    "            lemma = word\n",
    "        else:\n",
    "            lemma = wnl.lemmatize(word, wntag)\n",
    "        transformed_document+= lemma + \" \"\n",
    "    transformed_corpus += [transformed_document]\n",
    "\n",
    "df['transformed_sentence']=transformed_corpus\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1910998c",
   "metadata": {},
   "source": [
    "Text lemmatization, or breaking down words into their base or root form, is carried out by this code on the sentences included in the DataFrame df(sentence). NLTK library's WordNet Lemmatizer is used for the lemmatization process. The lemmatized phrases are initially stored in a blank list called transformed_corpus. By joining the lemmatized words together, it iteratively creates the changed phrase for each sentence in the DataFrame. To the list of converted corpuses, the transformed sentence is appended.\n",
    "At the end, it creates a new column in the DataFrame named transformed_sentence that has the sentences' lemmatized forms.\n",
    "Words are reduced to their most basic forms through the lemmatization process, which helps standardize the text data and enhances the efficiency of natural language processing activities like sentiment analysis and text classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d976bfa",
   "metadata": {},
   "source": [
    "#### Split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e2da8411",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['transformed_sentence']\n",
    "y = df['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7c7236",
   "metadata": {},
   "source": [
    "This code uses the train_test_split function from the scikit-learn library to divide the modified sentences (X) and their matching labels (y) into training and testing sets. We can train the model on a subset of the data and assess its performance on a different subset by dividing the dataset into training and testing sets. This helps prevent overfitting, which occurs when a model performs well on training data but badly on fresh, unseen data, and evaluates how well the model generalizes to new data. Thirty percent of the data will be utilized for testing and seventy percent will be used for training, according to the test_size=0.3 option. The size of the training and testing sets can be impacted by changing this parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab0fc7d",
   "metadata": {},
   "source": [
    "## TF-IDF vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f3e5aeed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aailiyah</th>\n",
       "      <th>ability</th>\n",
       "      <th>able</th>\n",
       "      <th>abound</th>\n",
       "      <th>abroad</th>\n",
       "      <th>absolute</th>\n",
       "      <th>absolutel</th>\n",
       "      <th>absolutely</th>\n",
       "      <th>abstruse</th>\n",
       "      <th>abysmal</th>\n",
       "      <th>...</th>\n",
       "      <th>younger</th>\n",
       "      <th>youthful</th>\n",
       "      <th>youtube</th>\n",
       "      <th>yucky</th>\n",
       "      <th>yukon</th>\n",
       "      <th>yummy</th>\n",
       "      <th>z</th>\n",
       "      <th>zero</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zombiez</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1918</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1919</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1920</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1921</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1922</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1923 rows × 3474 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      aailiyah  ability  able  abound  abroad  absolute  absolutel  \\\n",
       "0          0.0      0.0   0.0     0.0     0.0       0.0        0.0   \n",
       "1          0.0      0.0   0.0     0.0     0.0       0.0        0.0   \n",
       "2          0.0      0.0   0.0     0.0     0.0       0.0        0.0   \n",
       "3          0.0      0.0   0.0     0.0     0.0       0.0        0.0   \n",
       "4          0.0      0.0   0.0     0.0     0.0       0.0        0.0   \n",
       "...        ...      ...   ...     ...     ...       ...        ...   \n",
       "1918       0.0      0.0   0.0     0.0     0.0       0.0        0.0   \n",
       "1919       0.0      0.0   0.0     0.0     0.0       0.0        0.0   \n",
       "1920       0.0      0.0   0.0     0.0     0.0       0.0        0.0   \n",
       "1921       0.0      0.0   0.0     0.0     0.0       0.0        0.0   \n",
       "1922       0.0      0.0   0.0     0.0     0.0       0.0        0.0   \n",
       "\n",
       "      absolutely  abstruse  abysmal  ...  younger  youthful  youtube  yucky  \\\n",
       "0            0.0       0.0      0.0  ...      0.0       0.0      0.0    0.0   \n",
       "1            0.0       0.0      0.0  ...      0.0       0.0      0.0    0.0   \n",
       "2            0.0       0.0      0.0  ...      0.0       0.0      0.0    0.0   \n",
       "3            0.0       0.0      0.0  ...      0.0       0.0      0.0    0.0   \n",
       "4            0.0       0.0      0.0  ...      0.0       0.0      0.0    0.0   \n",
       "...          ...       ...      ...  ...      ...       ...      ...    ...   \n",
       "1918         0.0       0.0      0.0  ...      0.0       0.0      0.0    0.0   \n",
       "1919         0.0       0.0      0.0  ...      0.0       0.0      0.0    0.0   \n",
       "1920         0.0       0.0      0.0  ...      0.0       0.0      0.0    0.0   \n",
       "1921         0.0       0.0      0.0  ...      0.0       0.0      0.0    0.0   \n",
       "1922         0.0       0.0      0.0  ...      0.0       0.0      0.0    0.0   \n",
       "\n",
       "      yukon  yummy    z  zero  zombie  zombiez  \n",
       "0       0.0    0.0  0.0   0.0     0.0      0.0  \n",
       "1       0.0    0.0  0.0   0.0     0.0      0.0  \n",
       "2       0.0    0.0  0.0   0.0     0.0      0.0  \n",
       "3       0.0    0.0  0.0   0.0     0.0      0.0  \n",
       "4       0.0    0.0  0.0   0.0     0.0      0.0  \n",
       "...     ...    ...  ...   ...     ...      ...  \n",
       "1918    0.0    0.0  0.0   0.0     0.0      0.0  \n",
       "1919    0.0    0.0  0.0   0.0     0.0      0.0  \n",
       "1920    0.0    0.0  0.0   0.0     0.0      0.0  \n",
       "1921    0.0    0.0  0.0   0.0     0.0      0.0  \n",
       "1922    0.0    0.0  0.0   0.0     0.0      0.0  \n",
       "\n",
       "[1923 rows x 3474 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TfidfVectorizer includes pre-processing, tokenization, filtering stop words\n",
    "tfidf_vect = TfidfVectorizer(stop_words='english', lowercase=True, token_pattern=\"[^\\W\\d_]+\")\n",
    "\n",
    "X_train = tfidf_vect.fit_transform(X_train)\n",
    "X_test = tfidf_vect.transform(X_test)\n",
    "\n",
    "pd.DataFrame(X_train.toarray(), columns=tfidf_vect.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdcb9313",
   "metadata": {},
   "source": [
    "Above code segment uses scikit-learn's TF-IDF vectorizer (TfidfVectorizer) to eliminate stop words from the altered phrases. In natural language processing, TF-IDF vectorization is a widely used technique that turns text data into numerical vectors while accounting for the significance of each word in the corpus. In order to effectively train and evaluate machine learning models for tasks like sentiment analysis, classification, or any other predictive modeling activity, the text data must first undergo preprocessing and feature engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f11895",
   "metadata": {},
   "source": [
    "## Apply Singular Value Decomposition (SVD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "82b67a6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>svd0000</th>\n",
       "      <th>svd0001</th>\n",
       "      <th>svd0002</th>\n",
       "      <th>svd0003</th>\n",
       "      <th>svd0004</th>\n",
       "      <th>svd0005</th>\n",
       "      <th>svd0006</th>\n",
       "      <th>svd0007</th>\n",
       "      <th>svd0008</th>\n",
       "      <th>svd0009</th>\n",
       "      <th>...</th>\n",
       "      <th>svd0290</th>\n",
       "      <th>svd0291</th>\n",
       "      <th>svd0292</th>\n",
       "      <th>svd0293</th>\n",
       "      <th>svd0294</th>\n",
       "      <th>svd0295</th>\n",
       "      <th>svd0296</th>\n",
       "      <th>svd0297</th>\n",
       "      <th>svd0298</th>\n",
       "      <th>svd0299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.036036</td>\n",
       "      <td>0.004047</td>\n",
       "      <td>0.010477</td>\n",
       "      <td>-0.000212</td>\n",
       "      <td>0.023794</td>\n",
       "      <td>-0.024190</td>\n",
       "      <td>-0.005102</td>\n",
       "      <td>0.125046</td>\n",
       "      <td>-0.025465</td>\n",
       "      <td>-0.011163</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021773</td>\n",
       "      <td>0.027602</td>\n",
       "      <td>-0.017960</td>\n",
       "      <td>-0.016527</td>\n",
       "      <td>-0.057679</td>\n",
       "      <td>0.040319</td>\n",
       "      <td>-0.016026</td>\n",
       "      <td>0.013869</td>\n",
       "      <td>0.004335</td>\n",
       "      <td>0.018942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.012802</td>\n",
       "      <td>0.028359</td>\n",
       "      <td>0.012502</td>\n",
       "      <td>0.058239</td>\n",
       "      <td>0.027380</td>\n",
       "      <td>-0.046723</td>\n",
       "      <td>0.072406</td>\n",
       "      <td>0.030775</td>\n",
       "      <td>-0.006315</td>\n",
       "      <td>-0.009869</td>\n",
       "      <td>...</td>\n",
       "      <td>0.026913</td>\n",
       "      <td>-0.050628</td>\n",
       "      <td>0.006768</td>\n",
       "      <td>0.035223</td>\n",
       "      <td>-0.020398</td>\n",
       "      <td>-0.016046</td>\n",
       "      <td>-0.040581</td>\n",
       "      <td>0.024010</td>\n",
       "      <td>0.014855</td>\n",
       "      <td>0.015992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.045713</td>\n",
       "      <td>0.091588</td>\n",
       "      <td>-0.020400</td>\n",
       "      <td>-0.052102</td>\n",
       "      <td>0.022160</td>\n",
       "      <td>-0.102036</td>\n",
       "      <td>0.114591</td>\n",
       "      <td>-0.016682</td>\n",
       "      <td>-0.070260</td>\n",
       "      <td>-0.048706</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002968</td>\n",
       "      <td>0.026344</td>\n",
       "      <td>0.018515</td>\n",
       "      <td>-0.054598</td>\n",
       "      <td>-0.009361</td>\n",
       "      <td>0.059165</td>\n",
       "      <td>-0.019670</td>\n",
       "      <td>-0.007820</td>\n",
       "      <td>0.015184</td>\n",
       "      <td>-0.033229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.036720</td>\n",
       "      <td>0.027728</td>\n",
       "      <td>0.027896</td>\n",
       "      <td>-0.018806</td>\n",
       "      <td>0.082960</td>\n",
       "      <td>-0.026933</td>\n",
       "      <td>-0.037530</td>\n",
       "      <td>0.011056</td>\n",
       "      <td>-0.012303</td>\n",
       "      <td>0.021355</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.012977</td>\n",
       "      <td>0.023511</td>\n",
       "      <td>-0.006769</td>\n",
       "      <td>-0.032233</td>\n",
       "      <td>0.012983</td>\n",
       "      <td>0.028078</td>\n",
       "      <td>-0.002234</td>\n",
       "      <td>0.029054</td>\n",
       "      <td>0.013473</td>\n",
       "      <td>0.076427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.055962</td>\n",
       "      <td>0.137946</td>\n",
       "      <td>0.112265</td>\n",
       "      <td>0.418814</td>\n",
       "      <td>0.041115</td>\n",
       "      <td>-0.015961</td>\n",
       "      <td>0.113732</td>\n",
       "      <td>-0.042769</td>\n",
       "      <td>-0.079189</td>\n",
       "      <td>-0.137569</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009773</td>\n",
       "      <td>-0.010169</td>\n",
       "      <td>-0.022266</td>\n",
       "      <td>0.019233</td>\n",
       "      <td>0.008603</td>\n",
       "      <td>0.012213</td>\n",
       "      <td>0.004572</td>\n",
       "      <td>0.000153</td>\n",
       "      <td>-0.008426</td>\n",
       "      <td>0.020464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1918</th>\n",
       "      <td>0.041307</td>\n",
       "      <td>0.103656</td>\n",
       "      <td>-0.096002</td>\n",
       "      <td>-0.073437</td>\n",
       "      <td>0.012566</td>\n",
       "      <td>-0.171944</td>\n",
       "      <td>0.227098</td>\n",
       "      <td>-0.008403</td>\n",
       "      <td>-0.000868</td>\n",
       "      <td>0.054963</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027826</td>\n",
       "      <td>-0.036408</td>\n",
       "      <td>-0.000802</td>\n",
       "      <td>0.023307</td>\n",
       "      <td>-0.011283</td>\n",
       "      <td>0.014631</td>\n",
       "      <td>-0.001109</td>\n",
       "      <td>0.023998</td>\n",
       "      <td>0.004203</td>\n",
       "      <td>-0.010399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1919</th>\n",
       "      <td>0.056659</td>\n",
       "      <td>0.024515</td>\n",
       "      <td>0.028658</td>\n",
       "      <td>0.029943</td>\n",
       "      <td>0.035624</td>\n",
       "      <td>-0.034223</td>\n",
       "      <td>-0.005631</td>\n",
       "      <td>0.198052</td>\n",
       "      <td>-0.048357</td>\n",
       "      <td>-0.040169</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.014674</td>\n",
       "      <td>-0.003184</td>\n",
       "      <td>0.015108</td>\n",
       "      <td>0.004208</td>\n",
       "      <td>0.003222</td>\n",
       "      <td>0.009626</td>\n",
       "      <td>0.031377</td>\n",
       "      <td>0.008078</td>\n",
       "      <td>0.001283</td>\n",
       "      <td>0.001157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1920</th>\n",
       "      <td>0.074759</td>\n",
       "      <td>-0.007846</td>\n",
       "      <td>0.020618</td>\n",
       "      <td>0.007871</td>\n",
       "      <td>0.033606</td>\n",
       "      <td>-0.039924</td>\n",
       "      <td>-0.031925</td>\n",
       "      <td>0.289709</td>\n",
       "      <td>-0.056566</td>\n",
       "      <td>0.013666</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.033511</td>\n",
       "      <td>0.023786</td>\n",
       "      <td>0.018251</td>\n",
       "      <td>-0.040950</td>\n",
       "      <td>0.008795</td>\n",
       "      <td>0.011306</td>\n",
       "      <td>0.008002</td>\n",
       "      <td>-0.006215</td>\n",
       "      <td>-0.027792</td>\n",
       "      <td>-0.011165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1921</th>\n",
       "      <td>0.044818</td>\n",
       "      <td>0.053056</td>\n",
       "      <td>0.055661</td>\n",
       "      <td>0.064574</td>\n",
       "      <td>-0.012860</td>\n",
       "      <td>0.009161</td>\n",
       "      <td>0.005199</td>\n",
       "      <td>0.017559</td>\n",
       "      <td>0.058988</td>\n",
       "      <td>0.129803</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007720</td>\n",
       "      <td>-0.005633</td>\n",
       "      <td>-0.017176</td>\n",
       "      <td>0.013230</td>\n",
       "      <td>-0.038849</td>\n",
       "      <td>0.067878</td>\n",
       "      <td>-0.004271</td>\n",
       "      <td>-0.005677</td>\n",
       "      <td>0.007262</td>\n",
       "      <td>0.014433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1922</th>\n",
       "      <td>0.029616</td>\n",
       "      <td>0.048293</td>\n",
       "      <td>0.023454</td>\n",
       "      <td>-0.038960</td>\n",
       "      <td>0.004305</td>\n",
       "      <td>0.025634</td>\n",
       "      <td>-0.016521</td>\n",
       "      <td>-0.011852</td>\n",
       "      <td>-0.089200</td>\n",
       "      <td>-0.061754</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000447</td>\n",
       "      <td>0.030415</td>\n",
       "      <td>0.016244</td>\n",
       "      <td>0.038446</td>\n",
       "      <td>0.024034</td>\n",
       "      <td>-0.022793</td>\n",
       "      <td>-0.012519</td>\n",
       "      <td>0.029605</td>\n",
       "      <td>-0.077607</td>\n",
       "      <td>-0.001949</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1923 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       svd0000   svd0001   svd0002   svd0003   svd0004   svd0005   svd0006  \\\n",
       "0     0.036036  0.004047  0.010477 -0.000212  0.023794 -0.024190 -0.005102   \n",
       "1     0.012802  0.028359  0.012502  0.058239  0.027380 -0.046723  0.072406   \n",
       "2     0.045713  0.091588 -0.020400 -0.052102  0.022160 -0.102036  0.114591   \n",
       "3     0.036720  0.027728  0.027896 -0.018806  0.082960 -0.026933 -0.037530   \n",
       "4     0.055962  0.137946  0.112265  0.418814  0.041115 -0.015961  0.113732   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "1918  0.041307  0.103656 -0.096002 -0.073437  0.012566 -0.171944  0.227098   \n",
       "1919  0.056659  0.024515  0.028658  0.029943  0.035624 -0.034223 -0.005631   \n",
       "1920  0.074759 -0.007846  0.020618  0.007871  0.033606 -0.039924 -0.031925   \n",
       "1921  0.044818  0.053056  0.055661  0.064574 -0.012860  0.009161  0.005199   \n",
       "1922  0.029616  0.048293  0.023454 -0.038960  0.004305  0.025634 -0.016521   \n",
       "\n",
       "       svd0007   svd0008   svd0009  ...   svd0290   svd0291   svd0292  \\\n",
       "0     0.125046 -0.025465 -0.011163  ...  0.021773  0.027602 -0.017960   \n",
       "1     0.030775 -0.006315 -0.009869  ...  0.026913 -0.050628  0.006768   \n",
       "2    -0.016682 -0.070260 -0.048706  ...  0.002968  0.026344  0.018515   \n",
       "3     0.011056 -0.012303  0.021355  ... -0.012977  0.023511 -0.006769   \n",
       "4    -0.042769 -0.079189 -0.137569  ...  0.009773 -0.010169 -0.022266   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "1918 -0.008403 -0.000868  0.054963  ...  0.027826 -0.036408 -0.000802   \n",
       "1919  0.198052 -0.048357 -0.040169  ... -0.014674 -0.003184  0.015108   \n",
       "1920  0.289709 -0.056566  0.013666  ... -0.033511  0.023786  0.018251   \n",
       "1921  0.017559  0.058988  0.129803  ...  0.007720 -0.005633 -0.017176   \n",
       "1922 -0.011852 -0.089200 -0.061754  ... -0.000447  0.030415  0.016244   \n",
       "\n",
       "       svd0293   svd0294   svd0295   svd0296   svd0297   svd0298   svd0299  \n",
       "0    -0.016527 -0.057679  0.040319 -0.016026  0.013869  0.004335  0.018942  \n",
       "1     0.035223 -0.020398 -0.016046 -0.040581  0.024010  0.014855  0.015992  \n",
       "2    -0.054598 -0.009361  0.059165 -0.019670 -0.007820  0.015184 -0.033229  \n",
       "3    -0.032233  0.012983  0.028078 -0.002234  0.029054  0.013473  0.076427  \n",
       "4     0.019233  0.008603  0.012213  0.004572  0.000153 -0.008426  0.020464  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "1918  0.023307 -0.011283  0.014631 -0.001109  0.023998  0.004203 -0.010399  \n",
       "1919  0.004208  0.003222  0.009626  0.031377  0.008078  0.001283  0.001157  \n",
       "1920 -0.040950  0.008795  0.011306  0.008002 -0.006215 -0.027792 -0.011165  \n",
       "1921  0.013230 -0.038849  0.067878 -0.004271 -0.005677  0.007262  0.014433  \n",
       "1922  0.038446  0.024034 -0.022793 -0.012519  0.029605 -0.077607 -0.001949  \n",
       "\n",
       "[1923 rows x 300 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd = TruncatedSVD(n_components=300, n_iter=10) #n_components is the number of topics, which should be less than the number of features\n",
    "\n",
    "X_train= svd.fit_transform(X_train)\n",
    "X_test = svd.transform(X_test)\n",
    "\n",
    "pd.DataFrame(X_train, columns=[f\"svd{num:04}\" for num in range(0,X_train.shape[1])])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ca9b43",
   "metadata": {},
   "source": [
    "Singular Value Decomposition (SVD) is used in this code segment to reduce dimensionality and enhance model performance. SVD helps lessen the chance of overfitting and speed up computation by decreasing the dimensionality of the data while keeping the most important information. \n",
    "\n",
    "\n",
    "n_components is set to 300 to  to strike a balance between computational efficiency and the amount of variance from the original data that is retained in the reduced dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edfa2bd7",
   "metadata": {},
   "source": [
    "# Data Modelling "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6545067b",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "db9b4396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************\n",
      "Performace of Logistic Regression\n",
      "\n",
      "Accuracy Score:   0.7527272727272727\n",
      "Recall Score:     0.7377049180327869\n",
      "Precision Score:  0.773955773955774\n",
      "F1 Score:         0.7553956834532375\n",
      "************************************\n",
      "************************************\n",
      "Performace of Logistic Regression using Random Search\n",
      "\n",
      "Accuracy Score:   0.7442424242424243\n",
      "Recall Score:     0.7072599531615925\n",
      "Precision Score:  0.7783505154639175\n",
      "F1 Score:         0.7411042944785277\n",
      "************************************\n"
     ]
    }
   ],
   "source": [
    "logreg_model = LogisticRegression()\n",
    "logreg_model.fit(X_train,y_train)\n",
    "\n",
    "y_pred_log = logreg_model.predict(X_test)\n",
    "\n",
    "print(\"************************************\")\n",
    "print('Performace of Logistic Regression')\n",
    "print()\n",
    "print(f\"{'Accuracy Score: ':18}{accuracy_score(y_test, y_pred_log)}\")\n",
    "print(f\"{'Recall Score:':18}{recall_score(y_test, y_pred_log)}\")\n",
    "print(f\"{'Precision Score: ':18}{precision_score(y_test, y_pred_log)}\")\n",
    "print(f\"{'F1 Score: ':18}{f1_score(y_test, y_pred_log)}\")\n",
    "print(\"************************************\")\n",
    "\n",
    "\n",
    "param_grid_random = {\n",
    "    'C': np.logspace(-3, 3, 50), \n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'solver': ['liblinear'],\n",
    "    'max_iter': [100, 500],  \n",
    "}\n",
    "\n",
    "best_random_search_model_log = RandomizedSearchCV(\n",
    "    estimator=LogisticRegression(random_state=random_seed), \n",
    "    scoring='accuracy', \n",
    "    param_distributions=param_grid_random, \n",
    "    n_iter=100,  \n",
    "    cv=10, \n",
    "    verbose=0, \n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "_ = best_random_search_model_log.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred = best_random_search_model_log.predict(X_test)\n",
    "print(\"************************************\")\n",
    "print('Performace of Logistic Regression using Random Search')\n",
    "print()\n",
    "print(f\"{'Accuracy Score: ':18}{accuracy_score(y_test, y_pred)}\")\n",
    "print(f\"{'Recall Score:':18}{recall_score(y_test, y_pred)}\")\n",
    "print(f\"{'Precision Score: ':18}{precision_score(y_test, y_pred)}\")\n",
    "print(f\"{'F1 Score: ':18}{f1_score(y_test, y_pred)}\")\n",
    "print(\"************************************\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72572ff8",
   "metadata": {},
   "source": [
    "Logistic Regression model is trained without the need for hyperparameter tuning, and its performance is assessed using measures like accuracy, recall, precision, and F1 score.\n",
    "\n",
    "Next, it uses Randomized Search Cross-Validation to find the optimal set of Logistic Regression model hyperparameters. The model is retrained with the ideal hyperparameters following hyperparameter adjustment, and it is assessed once more using the same performance criteria.\n",
    "\n",
    "The benefit of hyperparameter optimization in enhancing model performance is demonstrated by comparing the performance of Logistic Regression with and without hyperparameter tuning.\n",
    "\n",
    "Overall, the code shows how to construct and optimize a Logistic Regression model for binary classification tasks in a methodical manner, emphasizing the significance of performance evaluation, hyperparameter adjustment, and preprocessing in machine learning processes.\n",
    "\n",
    "- Upon hyperparameter tuning the logistic regression accuracy has decreased from 0.784 to 0.7818.\n",
    "- The recall score remained the same for logistic regression has increased from 0.7528 to 0.7597 after hyperparameter tuning.\n",
    "- The precision score of the logistic regression has decreased from 0.8245 to 0.8157 after hyper parameter tuning.\n",
    "- The F1 score has improved slightly from 0.7870 to 0.7867 after hyperparameter tuning.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79086760",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ea4425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************\n",
      "Performace of SVM\n",
      "\n",
      "Accuracy Score:   0.7551515151515151\n",
      "Recall Score:     0.7564402810304449\n",
      "Precision Score:  0.7672209026128266\n",
      "F1 Score:         0.7617924528301887\n",
      "************************************\n"
     ]
    }
   ],
   "source": [
    "svm_model= SVC()\n",
    "svm_model.fit(X_train,y_train)\n",
    "\n",
    "y_pred_svm = svm_model.predict(X_test)\n",
    "\n",
    "print(\"************************************\")\n",
    "print('Performace of SVM')\n",
    "print()\n",
    "print(f\"{'Accuracy Score: ':18}{accuracy_score(y_test, y_pred_svm)}\")\n",
    "print(f\"{'Recall Score:':18}{recall_score(y_test, y_pred_svm)}\")\n",
    "print(f\"{'Precision Score: ':18}{precision_score(y_test, y_pred_svm)}\")\n",
    "print(f\"{'F1 Score: ':18}{f1_score(y_test, y_pred_svm)}\")\n",
    "print(\"************************************\")\n",
    "\n",
    "param_grid_random = {\n",
    "    'C': np.logspace(-3, 2, 10),    \n",
    "    'kernel': ['linear', 'rbf', 'poly'], \n",
    "    'gamma': np.logspace(-3, 2, 10)     \n",
    "}\n",
    "\n",
    "best_random_search_model_svm = RandomizedSearchCV(\n",
    "    estimator=SVC(random_state=random_seed), \n",
    "    scoring='accuracy', \n",
    "    param_distributions=param_grid_random, \n",
    "    n_iter=100, \n",
    "    cv=10, \n",
    "    verbose=0, \n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "_ = best_random_search_model_svm.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred = best_random_search_model_svm.predict(X_test)\n",
    "print(\"************************************\")\n",
    "print('Performace of SVM using Random Search')\n",
    "print()\n",
    "print(f\"{'Accuracy Score: ':18}{accuracy_score(y_test, y_pred)}\")\n",
    "print(f\"{'Recall Score:':18}{recall_score(y_test, y_pred)}\")\n",
    "print(f\"{'Precision Score: ':18}{precision_score(y_test, y_pred)}\")\n",
    "print(f\"{'F1 Score: ':18}{f1_score(y_test, y_pred)}\")\n",
    "print(\"************************************\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b4b9f2",
   "metadata": {},
   "source": [
    "Fits the SVM model to the training data (X_train, y_train) after initializing it with SVC(). uses the trained SVM model to predict the target values for the test data (X_test).\n",
    "calculates and prints metrics like Accuracy Score, Recall Score, Precision Score, and F1 Score to assess the effectiveness of the basic SVM model.\n",
    "\n",
    "Hyperparameter Tuning with Randomized Search is done crating a a parameter grid (param_grid_random) with various values for hyperparameters such as \"C,\" \"kernel,\" and \"gamma.\" establishes a Randomized Search Cross-Validation (RandomizedSearchCV) to determine the SVM model's optimal hyperparameters.\n",
    "\n",
    "- Upon hyperparameter tuning the SVM accuracy has increased from 0.764 to 0.775.\n",
    "- The recall score of the SVM has increased from 0.7438 to 0.7807 after hyper parameter tuning.\n",
    "- The precision score of the SVM has decreased from 0.7704 to 0.7675 after hyper parameter tuning.\n",
    "- The F1 score has improved slightly from 0.7568 to 0.7741 after hyperparameter tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7741c009",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae47e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtree_model = DecisionTreeClassifier()\n",
    "dtree_model.fit(X_train,y_train)\n",
    "\n",
    "y_pred_dtree = dtree_model.predict(X_test)\n",
    "\n",
    "print(\"************************************\")\n",
    "print('Performace of Decision Tree ')\n",
    "print()\n",
    "print(f\"{'Accuracy Score: ':18}{accuracy_score(y_test, y_pred_dtree)}\")\n",
    "print(f\"{'Recall Score:':18}{recall_score(y_test, y_pred_dtree)}\")\n",
    "print(f\"{'Precision Score: ':18}{precision_score(y_test, y_pred_dtree)}\")\n",
    "print(f\"{'F1 Score: ':18}{f1_score(y_test, y_pred_dtree)}\")\n",
    "print(\"************************************\")\n",
    "\n",
    "\n",
    "criterion = ['gini', 'entropy']\n",
    "max_depth = [int(x) for x in np.linspace(1, 1000, 20)] \n",
    "max_depth.append(None)\n",
    "min_samples_split = [int(x) for x in np.linspace(2, 100, 20)]  \n",
    "min_samples_leaf = [int(x) for x in np.linspace(1, 50, 20)]  \n",
    "max_leaf_nodes = [int(x) for x in np.linspace(2, 100, 20)]  \n",
    "max_leaf_nodes.append(None)\n",
    "min_impurity_decrease = [x for x in np.arange(0.0, 0.01, 0.0001).round(5)]  \n",
    "\n",
    "param_grid_random = {\n",
    "    'criterion': criterion,\n",
    "    'max_depth': max_depth,\n",
    "    'min_samples_split': min_samples_split,\n",
    "    'min_samples_leaf' : min_samples_leaf,\n",
    "    'max_leaf_nodes' : max_leaf_nodes,\n",
    "    'min_impurity_decrease' : min_impurity_decrease,\n",
    "}\n",
    "\n",
    "best_random_search_model_dt = RandomizedSearchCV(\n",
    "    estimator=DecisionTreeClassifier(random_state=random_seed), \n",
    "    scoring='accuracy', \n",
    "    param_distributions=param_grid_random, \n",
    "    n_iter=100,\n",
    "    cv=10, \n",
    "    verbose=0, \n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "_ = best_random_search_model_dt.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "y_pred = best_random_search_model_dt.predict(X_test)\n",
    "print(\"************************************\")\n",
    "print('Performace of Decision Tree using Random Search')\n",
    "print()\n",
    "print(f\"{'Recall Score:':18}{recall_score(y_test, y_pred)}\")\n",
    "print(f\"{'Accuracy Score: ':18}{accuracy_score(y_test, y_pred)}\")\n",
    "print(f\"{'Precision Score: ':18}{precision_score(y_test, y_pred)}\")\n",
    "print(f\"{'F1 Score: ':18}{f1_score(y_test, y_pred)}\")\n",
    "print(\"************************************\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62de6cc0",
   "metadata": {},
   "source": [
    "A Decision Tree is a flowchart-like tree structure where an internal node represents a feature(or attribute), the branch represents a decision rule, and each leaf node represents the outcome. Using the training data, they derive basic decision-making rules to forecast the target variable.\n",
    "\n",
    "Hyperparameters are those that are chosen before the model is trained. The process of hyperparameter tuning entails determining the best set of hyperparameters to increase model performance.\n",
    "Techniques:\n",
    "Randomized Search: Selects hyperparameters at random from predefined distributions and assesses how well they work.\n",
    "\n",
    "Hyperparameters of Decision Trees:\n",
    "Criteria: Indicates a split's quality (e.g., entropy or Gini impurity).\n",
    "Max Depth: The tree's maximum depth.\n",
    "The bare minimum of samples needed to separate an internal node is known as \"Min Samples Split.\"\n",
    "Min Leaf Samples: The bare minimum of samples necessary for a leaf node.\n",
    "Maximum Leaf Nodes: The highest quantity of leaf nodes within the tree.\n",
    "Min Impurity Decrease: The least amount of impurities needed for a split to occur\n",
    "\n",
    "- Upon hyperparameter tuning the Decision tree accuracy has increased from 0.64 to 0.704.\n",
    "- The recall score of the decision tree has decreased from 0.635 to 0.632 after hyper parameter tuning.\n",
    "- The precision score of the decision tree has decreased from 0.6339 to 0.6098 after hyper parameter tuning.\n",
    "- The F1 score has improved slightly from 0.6346 to 0.6537 after hyperparameter tuning the model.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d9d78f",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "39fa1bfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************\n",
      "Performace of KNN \n",
      "\n",
      "Accuracy Score:   0.6921212121212121\n",
      "Recall Score:     0.679814385150812\n",
      "Precision Score:  0.7163814180929096\n",
      "F1 Score:         0.6976190476190476\n",
      "************************************\n",
      "************************************\n",
      "Performance of K-Nearest Neighbors using Random Search\n",
      "\n",
      "Accuracy Score:   0.7054545454545454\n",
      "Recall Score:     0.6937354988399071\n",
      "Precision Score:  0.7292682926829268\n",
      "F1 Score:         0.7110582639714625\n",
      "************************************\n"
     ]
    }
   ],
   "source": [
    "knn_model = KNeighborsClassifier()\n",
    "\n",
    "knn_model.fit(X_train, y_train)\n",
    "y_pred_knn = knn_model.predict(X_test)\n",
    "\n",
    "print(\"************************************\")\n",
    "print('Performace of KNN ')\n",
    "print()\n",
    "print(f\"{'Accuracy Score: ':18}{accuracy_score(y_test, y_pred_knn)}\")\n",
    "print(f\"{'Recall Score:':18}{recall_score(y_test, y_pred_knn)}\")\n",
    "print(f\"{'Precision Score: ':18}{precision_score(y_test, y_pred_knn)}\")\n",
    "print(f\"{'F1 Score: ':18}{f1_score(y_test, y_pred_knn)}\")\n",
    "print(\"************************************\")\n",
    "\n",
    "param_grid_knn = {\n",
    "    'n_neighbors': range(1, 21), \n",
    "    'weights': ['uniform', 'distance'],  \n",
    "    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],  \n",
    "    'p': [1, 2]  \n",
    "}\n",
    "\n",
    "best_random_search_model_knn = RandomizedSearchCV(\n",
    "    estimator=KNeighborsClassifier(), \n",
    "    param_distributions=param_grid_knn, \n",
    "    n_iter=100,  \n",
    "    cv=10, \n",
    "    scoring='accuracy', \n",
    "    verbose=0, \n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "_ = best_random_search_model_knn.fit(X_train, y_train)\n",
    "\n",
    "y_pred_knn_best = best_random_search_model_knn.predict(X_test)\n",
    "# Print performance metrics\n",
    "print(\"************************************\")\n",
    "print('Performance of K-Nearest Neighbors using Random Search')\n",
    "print()\n",
    "print(f\"{'Accuracy Score: ':18}{accuracy_score(y_test, y_pred_knn_best)}\")\n",
    "print(f\"{'Recall Score:':18}{recall_score(y_test, y_pred_knn_best)}\")\n",
    "print(f\"{'Precision Score: ':18}{precision_score(y_test, y_pred_knn_best)}\")\n",
    "print(f\"{'F1 Score: ':18}{f1_score(y_test, y_pred_knn_best)}\")\n",
    "print(\"************************************\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553988c7",
   "metadata": {},
   "source": [
    "The similarity concept forms the foundation of KNN. It is predicated on the idea that data points with comparable numerical values or classes tend to be similar.\n",
    "KNN uses the 'k' closest data points (nearest neighbors) in the feature space to generate predictions for a new data point. For classification purposes, this means assigning the majority class, and for regression, the average value of these neighbors to the new data point.\n",
    "\n",
    "The KNN model's ideal hyperparameters are found via randomized search.\n",
    "The weight function (weights), the technique used to find the nearest neighbors (algorithm), the power parameter for the Minkowski distance ('p'), and the number of neighbors ('n_neighbors') are examples of hyperparameters.\n",
    "\n",
    "The test dataset's instance classification accuracy is used to evaluate the KNN model's performance.\n",
    "\n",
    "- Upon hyperparameter tuning the KNN accuracy has increased slightly from 0.6436 to 0.6545.\n",
    "- The recall score of the KNN has increased from 0.5246 to 0.5714 after hyper parameter tuning.\n",
    "- The precision score of the KNN has decreased slightly from 0.6783 to 0.6763 after hyper parameter tuning.\n",
    "- The F1 score has improved  from 0.5916 to 0.6194 after hyperparameter tuning the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0978a52",
   "metadata": {},
   "source": [
    "## AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3e836e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************\n",
      "Performace of AdaBoost \n",
      "\n",
      "Accuracy Score:   0.6957575757575758\n",
      "Recall Score:     0.6751740139211136\n",
      "Precision Score:  0.7238805970149254\n",
      "F1 Score:         0.6986794717887154\n",
      "************************************\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 33\u001b[0m\n\u001b[0;32m     16\u001b[0m param_grid_adaboost \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_estimators\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m50\u001b[39m, \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m200\u001b[39m, \u001b[38;5;241m300\u001b[39m],  \n\u001b[0;32m     18\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39mlogspace(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m50\u001b[39m), \n\u001b[0;32m     19\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124malgorithm\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSAMME\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSAMME.R\u001b[39m\u001b[38;5;124m'\u001b[39m]  \n\u001b[0;32m     20\u001b[0m }\n\u001b[0;32m     23\u001b[0m best_random_search_model_adaboost \u001b[38;5;241m=\u001b[39m RandomizedSearchCV(\n\u001b[0;32m     24\u001b[0m     estimator\u001b[38;5;241m=\u001b[39mAdaBoostClassifier(), \n\u001b[0;32m     25\u001b[0m     param_distributions\u001b[38;5;241m=\u001b[39mparam_grid_adaboost, \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     30\u001b[0m     n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     31\u001b[0m )\n\u001b[1;32m---> 33\u001b[0m _ \u001b[38;5;241m=\u001b[39m \u001b[43mbest_random_search_model_adaboost\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m y_pred_adaboost_best \u001b[38;5;241m=\u001b[39m best_random_search_model_adaboost\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m************************************\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:898\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    892\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m    893\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    894\u001b[0m     )\n\u001b[0;32m    896\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m--> 898\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    900\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    901\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    902\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1806\u001b[0m, in \u001b[0;36mRandomizedSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1805\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1806\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1807\u001b[0m \u001b[43m        \u001b[49m\u001b[43mParameterSampler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1808\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_distributions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom_state\u001b[49m\n\u001b[0;32m   1809\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1810\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:845\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    837\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    838\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    839\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    840\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    841\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[0;32m    842\u001b[0m         )\n\u001b[0;32m    843\u001b[0m     )\n\u001b[1;32m--> 845\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    846\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    847\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    848\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    849\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    850\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    851\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    852\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    853\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    854\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    855\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    856\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    857\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    858\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    859\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    860\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    862\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    863\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    864\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    865\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    866\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    867\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     60\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     61\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     64\u001b[0m )\n\u001b[1;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:1098\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1095\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1097\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1098\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;66;03m# Make sure that we get a last message telling us we are done\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m elapsed_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_time\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:975\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    973\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    974\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msupports_timeout\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m--> 975\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(\u001b[43mjob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    976\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    977\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(job\u001b[38;5;241m.\u001b[39mget())\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\joblib\\_parallel_backends.py:567\u001b[0m, in \u001b[0;36mLokyBackend.wrap_future_result\u001b[1;34m(future, timeout)\u001b[0m\n\u001b[0;32m    564\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Wrapper for Future.result to implement the same behaviour as\u001b[39;00m\n\u001b[0;32m    565\u001b[0m \u001b[38;5;124;03mAsyncResults.get from multiprocessing.\"\"\"\u001b[39;00m\n\u001b[0;32m    566\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 567\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    568\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m CfTimeoutError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    569\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\concurrent\\futures\\_base.py:451\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m    449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[1;32m--> 451\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_condition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[0;32m    454\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[0;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 320\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    321\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    322\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "adaboost_model = AdaBoostClassifier()\n",
    "\n",
    "adaboost_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_adaboost = adaboost_model.predict(X_test)\n",
    "\n",
    "print(\"************************************\")\n",
    "print('Performace of AdaBoost ')\n",
    "print()\n",
    "print(f\"{'Accuracy Score: ':18}{accuracy_score(y_test, y_pred_adaboost)}\")\n",
    "print(f\"{'Recall Score:':18}{recall_score(y_test, y_pred_adaboost)}\")\n",
    "print(f\"{'Precision Score: ':18}{precision_score(y_test, y_pred_adaboost)}\")\n",
    "print(f\"{'F1 Score: ':18}{f1_score(y_test, y_pred_adaboost)}\")\n",
    "print(\"************************************\")\n",
    "\n",
    "param_grid_adaboost = {\n",
    "    'n_estimators': [50, 100, 200, 300],  \n",
    "    'learning_rate': np.logspace(-3, 0, 50), \n",
    "    'algorithm': ['SAMME', 'SAMME.R']  \n",
    "}\n",
    "\n",
    "\n",
    "best_random_search_model_adaboost = RandomizedSearchCV(\n",
    "    estimator=AdaBoostClassifier(), \n",
    "    param_distributions=param_grid_adaboost, \n",
    "    n_iter=50,  \n",
    "    cv=5, \n",
    "    scoring='accuracy', \n",
    "    verbose=0, \n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "_ = best_random_search_model_adaboost.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred_adaboost_best = best_random_search_model_adaboost.predict(X_test)\n",
    "\n",
    "print(\"************************************\")\n",
    "print('Performance of AdaBoost using Random Search')\n",
    "print()\n",
    "print(f\"{'Accuracy Score: ':18}{accuracy_score(y_test, y_pred_adaboost_best)}\")\n",
    "print(f\"{'Recall Score:':18}{recall_score(y_test, y_pred_adaboost_best)}\")\n",
    "print(f\"{'Precision Score: ':18}{precision_score(y_test, y_pred_adaboost_best)}\")\n",
    "print(f\"{'F1 Score: ':18}{f1_score(y_test, y_pred_adaboost_best)}\")\n",
    "print(\"************************************\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f4e86e",
   "metadata": {},
   "source": [
    "AdaBoost is an ensemble learning technique that builds a powerful classifier by combining several weak learners, usually decision trees.\n",
    "It operates by fitting a sequence of weak learners in a sequential manner on the training set, with each new learner concentrating on the cases that the earlier ones misclassified.\n",
    "\n",
    "At the beginning, every instance in the training set has the same weight.\n",
    "Using the weighted training data, a weak learner is trained and its performance assessed.\n",
    "Higher weights are given to cases that the weak learner incorrectly classifies, while lower weights are given to examples that are successfully classified.\n",
    "Each succeeding weak learner concentrates more on the previously incorrectly classified cases when the process is repeated a predetermined number of times (or until the required accuracy is attained).\n",
    "\n",
    "The main hyperparameters of AdaBoost include the number of weak learners (n_estimators = 50, 100, 200, 300) and the learning rate (learning_rate : np.logspace(-3, 0, 50), ), which controls the contribution of each weak learner to the final prediction.\n",
    "\n",
    "The performance of the Model is stated below -\n",
    "\n",
    "- Upon hyperparameter tuning the Adaboost accuracy has increased from 0.6945 to 0.724.\n",
    "- The recall score of the Adaboost has increased from 0.677 to 0.714 after hyper parameter tuning.\n",
    "- The precision score of the Adaboost has increased from 0.6944 to 0.7231 after hyper parameter tuning.\n",
    "- The F1 score has improved slightly from 0.6857 to 0.7187 after hyperparameter tuning the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c870ce6",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a3d40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = xgb.XGBClassifier()\n",
    "\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "\n",
    "\n",
    "print(\"************************************\")\n",
    "print('Performance of AdaBoost ')\n",
    "print()\n",
    "print(f\"{'Accuracy Score: ':18}{accuracy_score(y_test, y_pred_xgb)}\")\n",
    "print(f\"{'Recall Score:':18}{recall_score(y_test, y_pred_xgb)}\")\n",
    "print(f\"{'Precision Score: ':18}{precision_score(y_test, y_pred_xgb)}\")\n",
    "print(f\"{'F1 Score: ':18}{f1_score(y_test, y_pred_xgb)}\")\n",
    "print(\"************************************\")\n",
    "\n",
    "\n",
    "param_grid_xgb = {\n",
    "    'n_estimators': [50, 100, 200, 300], \n",
    "    'learning_rate': np.logspace(-3, 0, 50),  \n",
    "    'max_depth': [3, 4, 5, 6],  \n",
    "    'subsample': [0.6, 0.7, 0.8, 0.9, 1.0],  \n",
    "    'colsample_bytree': [0.6, 0.7, 0.8, 0.9, 1.0], \n",
    "    'gamma': [0, 0.1, 0.2, 0.3, 0.4]  \n",
    "}\n",
    "\n",
    "best_random_search_model_xgb = RandomizedSearchCV(\n",
    "    estimator=xgb.XGBClassifier(), \n",
    "    param_distributions=param_grid_xgb, \n",
    "    n_iter=50,  \n",
    "    cv=5, \n",
    "    scoring='accuracy', \n",
    "    verbose=0, \n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "_ = best_random_search_model_xgb.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred_xgb_best = best_random_search_model_xgb.predict(X_test)\n",
    "\n",
    "print(\"************************************\")\n",
    "print('Performance of XGBoost using Random Search')\n",
    "print()\n",
    "print(f\"{'Accuracy Score: ':18}{accuracy_score(y_test, y_pred_xgb_best)}\")\n",
    "print(f\"{'Recall Score:':18}{recall_score(y_test, y_pred_xgb_best)}\")\n",
    "print(f\"{'Precision Score: ':18}{precision_score(y_test, y_pred_xgb_best)}\")\n",
    "print(f\"{'F1 Score: ':18}{f1_score(y_test, y_pred_xgb_best)}\")\n",
    "print(\"************************************\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a89402",
   "metadata": {},
   "source": [
    "XGBoost is an ensemble learning technique that builds predictive models, primarily decision trees, using a gradient boosting framework.\n",
    "With a focus on speed and performance, it is an extremely effective and streamlined version of gradient boosting.\n",
    "\n",
    "Gradient Boosting: XGBoost trains a sequence of decision trees one after the other in an effort to fix the mistakes the earlier trees made.\n",
    "The objective function of XGBoost is to minimize a given loss function by adding new trees that lower the residual errors. Typically, this loss function is the log loss for classification problems and the mean squared error for regression problems.\n",
    "Regularization: To manage model complexity and avoid overfitting, XGBoost incorporates L1 (Lasso) and L2 (Ridge) regularization terms.\n",
    "\n",
    "The number of trees (n_estimators: 50, 100, 200, 300), the learning rate (learning_rate:  np.logspace(-3, 0, 50)), which regulates each tree's contribution, and the maximum depth of each tree (max_depth: 3, 4, 5, 6) are important hyperparameters.\n",
    "Subsample (the percentage of samples used to train each tree), colsample_bytree (the percentage of features used to train each tree), and gamma (the minimal loss reduction needed to perform a subsequent partition) are additional critical hyperparameters.\n",
    "\n",
    "- Upon hyperparameter tuning the XGboost accuracy has increased from 0.7224 to 0.7406.\n",
    "- The recall score of the XGboost has increased slightly from 0.7093 to 0.7345 after hyper parameter tuning.\n",
    "- The precision score of the XGboost has increased slightly from 0.7524 to 0.7661 after hyper parameter tuning.\n",
    "- The F1 score has decreased slightly from 0.7302 to 0.75 after hyperparameter tuning the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2203210a",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087dc5cf",
   "metadata": {},
   "source": [
    "The three files were merged into a single dataframe, and text analysis was done on it. After first using text lemmatization to condense the words to their most basic form, the data is divided and subjected to TFID vectorization. Tokenization, stop word filtering, and data preparation are all included in TFIDF vectorization. To reduce the likelihood of overfitting, expedite computing, and fit the data into the appropriate model, the data is subjected to single value decomposition. This allows for the achievement of performance metrics through the use of hybrid parameter tweaking, which yields the optimal model analysis.\n",
    "\n",
    "#### Logistic Regression\n",
    "\n",
    "- Upon hyperparameter tuning the logistic regression accuracy has decreased from 0.784 to 0.7818.\n",
    "- The recall score remained the same for logistic regression has increased from 0.7528 to 0.7597 after hyperparameter tuning.\n",
    "- The precision score of the logistic regression has decreased from 0.8245 to 0.8157 after hyper parameter tuning.\n",
    "- The F1 score has improved slightly from 0.7870 to 0.7867 after hyperparameter tuning.\n",
    "\n",
    "#### SVM\n",
    "\n",
    "- Upon hyperparameter tuning the SVM accuracy has increased from 0.764 to 0.775.\n",
    "- The recall score of the SVM has increased from 0.7438 to 0.7807 after hyper parameter tuning.\n",
    "- The precision score of the SVM has decreased from 0.7704 to 0.7675 after hyper parameter tuning.\n",
    "- The F1 score has improved slightly from 0.7568 to 0.7741 after hyperparameter tuning.\n",
    "\n",
    "#### Decision Tree\n",
    "\n",
    "- Upon hyperparameter tuning the Decision tree accuracy has increased from 0.64 to 0.704.\n",
    "- The recall score of the decision tree has decreased from 0.635 to 0.632 after hyper parameter tuning.\n",
    "- The precision score of the decision tree has decreased from 0.6339 to 0.6098 after hyper parameter tuning.\n",
    "- The F1 score has improved slightly from 0.6346 to 0.6537 after hyperparameter tuning the model.\n",
    "\n",
    "\n",
    "#### KNN\n",
    "\n",
    "- Upon hyperparameter tuning the KNN accuracy has increased slightly from 0.6436 to 0.6545.\n",
    "- The recall score of the KNN has increased from 0.5246 to 0.5714 after hyper parameter tuning.\n",
    "- The precision score of the KNN has decreased slightly from 0.6783 to 0.6763 after hyper parameter tuning.\n",
    "- The F1 score has improved  from 0.5916 to 0.6194 after hyperparameter tuning the model.\n",
    "\n",
    "#### Adaboost\n",
    "\n",
    "- Upon hyperparameter tuning the Adaboost accuracy has increased from 0.6945 to 0.724.\n",
    "- The recall score of the Adaboost has increased from 0.677 to 0.714 after hyper parameter tuning.\n",
    "- The precision score of the Adaboost has increased from 0.6944 to 0.7231 after hyper parameter tuning.\n",
    "- The F1 score has improved slightly from 0.6857 to 0.7187 after hyperparameter tuning the model.\n",
    "\n",
    "#### XGboost\n",
    "\n",
    "- Upon hyperparameter tuning the XGboost accuracy has increased from 0.7224 to 0.7406.\n",
    "- The recall score of the XGboost has increased slightly from 0.7093 to 0.7345 after hyper parameter tuning.\n",
    "- The precision score of the XGboost has increased slightly from 0.7524 to 0.7661 after hyper parameter tuning.\n",
    "- The F1 score has decreased slightly from 0.7302 to 0.75 after hyperparameter tuning the model.\n",
    "\n",
    "\n",
    "\n",
    "Since we are practically using data from three different sources the accuracies of the models is maximized at 0.7818 by logistic regression with hyperparameter tuning thus indicating the complexity of model optimization when interpretability is considered.\n",
    "\n",
    "XGBoost emerges as the best model overall, given its across-the-board improvements in accuracy, recall, precision, and F1 score.This suggests it is a robust model capable of handling various types of data and making accurate predictions with a good balance between identifying positive cases and minimizing false positives. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23dff3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
